{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hello, our names are Vincent and J. Vincent Shorter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Objective \n",
    "* This project runs through the entire Data Science Pipeline and culminates with classification modelling techniques based upon Natural Langauge Processing outcomes.\n",
    "* Pproject constructs a ML model to predict a github repository coding language based on just the project README file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analysis of the data showed that Python was by far the most popular language\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import model\n",
    "import wrangle\n",
    "import nltk\n",
    "\n",
    "from importlib import reload\n",
    "reload(model)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle (Acquire and Prep)\n",
    "* By far most difficult and laborious portion of project. \n",
    "* Had to contend with empty, non-English, and often sparse READMEs\n",
    "\n",
    "### Nulls/Missing Values\n",
    "* Simple drop of null values as they most often indicated an empty README\n",
    "---\n",
    "### Feature Engineering \n",
    "* Engineered `word_count` in order to facilitate analysis around column length and unique word density\n",
    "* Engineered `language_bigrams` in order to capture most used word duos\n",
    "---   \n",
    "### Flattening\n",
    "* Had to make decisions in order to remove optionality from language column due to sample size \n",
    "- Went from around 17 languages down to 7 by creating an `other` category for the less popular langauges\n",
    "* Decisons here driven mostly by desire to have enough observations for effective analysis \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Questions \n",
    "* Includes visualizations and statistical tests\n",
    "* Primarily focused on analyzing frequency differences between language groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling prepare/acquire functions from module to acquire and split data subsets\n",
    "df = wrangle.get_search_csv()\n",
    "df = wrangle.prep_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotlight - Common Words \n",
    "* **Question:** What are the most common words in READMEs?\n",
    "* **Answer:** \n",
    "\n",
    "#### Statistical Hypothesis\n",
    ">* ${H_0}$: There is no relationship between industry of typical employment and employment status   \n",
    ">* ${H_a}$: There is a relationship between industry of typical employment and employment status  \n",
    ">* ${\\alpha}$: .05  \n",
    ">* Result: There is enough evidence to reject our null hypothesis. **Test code below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotlight - README Length\n",
    "* **Question:** Does the length of the README vary by programming language?\n",
    "\n",
    "* **Answer:** Indivduals identifying as White show the largest population proportion change with a drop of nearly 10% when comparing employed vs unemployed. Those identifying as mixed race other than with white, and Indigenous have the highest unemployed rates at 12% and 7% respectively. \n",
    "\n",
    "#### Statistical Hypothesis\n",
    ">* ${H_0}$: There is no relationship between `race` and `employment` status   \n",
    ">* ${H_a}$: There is a relationship between `race` and `employment` status   \n",
    ">* ${\\alpha}$: .05  \n",
    ">* Result: There is enough evidence to reject our null hypothesis. **Test code below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotlight - Unique Words\n",
    "* **Question:** Do different programming languages use a different number of unique words?\n",
    " \n",
    "* **Answer:** \n",
    "\n",
    "#### Statistical Hypothesis\n",
    ">* ${H_0}$: There is no relationship between having a `professional_certification` and `employment`  \n",
    ">* ${H_a}$: There is a relationship between having a `professional_certification` and `employment`    \n",
    ">* ${\\alpha}$: .05\n",
    ">* Result: There is enough evidence to reject our null hypothesis. **Test code below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotlight - Language ID by Word\n",
    "* **Question:** Are there any words that uniquely identify a programming language?\n",
    " \n",
    "* **Answer:** \n",
    "\n",
    "#### Statistical Hypothesis\n",
    ">* ${H_0}$: There is no relationship between having a `professional_certification` and `employment`  \n",
    ">* ${H_a}$: There is a relationship between having a `professional_certification` and `employment`    \n",
    ">* ${\\alpha}$: .05\n",
    ">* Result: There is enough evidence to reject our null hypothesis. **Test code below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Summary\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "- Things did not go as plan. Initially had massive perfomanc drops moving in Validation\n",
    "- Use of custom class proved to be more of a hindrance than help\n",
    "- Had to abandon gridsearch idea, and focus on feature creation\n",
    "- Logistic Regression never provided much performance gain above baseline\n",
    "- DTC models consistenly peformed well, and, along with RF, we started lowering depth to control for overfitting \n",
    "- We did 5 rounds of mass cohort testing before settling on specific hyperparameters\n",
    "- Final Models had 37% performamce gain above baseline when scoring with Accuracy as focus\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_placement(language):\n",
    "    \"\"\" \n",
    "    Purpose:\n",
    "        to create bigrams for each langauge\n",
    "    ---\n",
    "    Parameters:\n",
    "        language: input language\n",
    "    ---\n",
    "    Returns:\n",
    "        a list of bigrams appropriately joined for further use\n",
    "    \"\"\"\n",
    "    # this initilizes the class objects. \n",
    "    html = model.code_language(words=' '.join(df[df.language == 'html'].lemmatized), label='html')\n",
    "    javascript = model.code_language(words=' '.join(df[df.language == 'javascript'].lemmatized), label='javascript')\n",
    "    r_ = model.code_language(words=' '.join(df[df.language == 'r'].lemmatized), label='r')\n",
    "    other_ = model.code_language(words=' '.join(df[df.language == 'other'].lemmatized), label='other')\n",
    "    python_ = model.code_language(words=' '.join(df[df.language == 'python'].lemmatized), label='python')\n",
    "    all_ = model.code_language(words=' '.join(df.lemmatized), label='all languages')\n",
    "\n",
    "    #creates bigrams for seperate languages\n",
    "    if language == 'html':\n",
    "        language = html.bigrams()\n",
    "    elif language == 'javascript':\n",
    "        language = javascript.bigrams()\n",
    "    elif language == 'r':\n",
    "        language = r_.bigrams()\n",
    "    elif language == 'python':\n",
    "        language = python_.bigrams()\n",
    "    else:\n",
    "        language = other_.bigrams()\n",
    "    return ', '.join(str(e) for e in language.str.join(sep=' ').to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy(Score)</th>\n",
       "      <th>Type</th>\n",
       "      <th>Features Used</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.402299</td>\n",
       "      <td>Basic Baseline</td>\n",
       "      <td>Baseline Prediction</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DTC_0</td>\n",
       "      <td>0.777800</td>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>['word_count', 'lemmatized', 'language_bigrams']</td>\n",
       "      <td>Depth: 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Accuracy(Score)                      Type  \\\n",
       "0  Baseline         0.402299            Basic Baseline   \n",
       "1     DTC_0         0.777800  Decision Tree Classifier   \n",
       "\n",
       "                                      Features Used Parameters  \n",
       "0                               Baseline Prediction        n/a  \n",
       "1  ['word_count', 'lemmatized', 'language_bigrams']   Depth: 5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies bigram function to new column\n",
    "df.language_bigrams = df.language_bigrams.apply(bigram_placement)\n",
    "\n",
    "#grabs data subsets after vectorization and splitting\n",
    "train, X_train, y_train, X_validate, y_validate, X_test, y_test = model.vectorize_split(df)\n",
    "\n",
    "#places subsets in data for import into final function\n",
    "subsets = [train, X_train, y_train, X_validate, y_validate, X_test, y_test]\n",
    "\n",
    "#final function to grab model scores\n",
    "model.get_test_score(df, subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for Modeling\n",
    "* Grouped by simple features: `word_count`, `lemmatized`, `language_bigrams`\n",
    "\n",
    "* `word_count` - engineered\n",
    "    * Chosen to highlight the business oriented concerns around employement  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* `lemmatized`\n",
    "    * Highlights family and environment characteristics  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `language_bigrams` - engineered\n",
    "    * Highlights word bigrams popular within each language  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Exposed Top 5 models from train section to the Validate data\n",
    "* Models had to perform underneath 93% on train set to control for possible overfitting\n",
    "* DTC models maintained highest performance profile througout all testing cohorts\n",
    "---\n",
    "* Model performace on Train subset:\n",
    "\n",
    "Model\t| Accuracy(Score)   |Type                       |Features Used                           |Parameters             |\n",
    "|---    | ---               |---                        |   -----                                |---                    |\n",
    "|RF_6   |\t0.91950         |Random Forest              |word_count, lemmatized, language_bigrams|Depth: 7, Leaves: 3\n",
    "|RF_0   |\t0.91950         |Random Forest              |word_count, lemmatized, language_bigrams|Depth: 5, Leaves: 3\n",
    "|DTC_1  |   0.91950         |Decision Tree Classifier   |word_count, lemmatized, language_bigrams|Depth: 6\n",
    "|DTC_0  |\t0.89066\t        |Decision Tree Classifier\t|word_count, lemmatized, language_bigrams|Depth: 5\n",
    "|KNN_4  |\t0.56322\t        |K-Nearest Neighbors\t    |word_count, lemmatized, language_bigrams|K-Neighbors: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test\n",
    "* Top Model Performance on Test set:  \n",
    "\n",
    "|    | Model    |   Accuracy(Score) | Type                     | Features Used                                    | Parameters   |\n",
    "|---:|:---------|------------------:|:-------------------------|:-------------------------------------------------|:-------------|\n",
    "|  0 | Baseline |          0.402299 | Basic Baseline           | Baseline Prediction                              | n/a          |\n",
    "|  1 | DTC_0    |          0.7778   | Decision Tree Classifier | ['word_count', 'lemmatized', 'language_bigrams'] | Depth: 5     |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "## Summary of Key Findings\n",
    "* \n",
    "* \n",
    "* \n",
    "* DTC and RF models consistenly performed well\n",
    "* Final Models had 37% performamce gain above baseline\n",
    "---\n",
    "## Suggestions and Next Steps\n",
    "* Trigrams may be something worth adding in the model in order to boost performance\n",
    "* We also want to create a form of sentiment analysis \n",
    "    - It will track whether a repo leans more towards Basketball or Coding as a focal point\n",
    "* Model performance above baseline is enough to justify continued use.\n",
    "* An affirmative next step would be to further expand the scope of testing to capture languages with smaller usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
