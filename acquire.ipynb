{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to scrape articles from one topic\n",
    "def scrape_one_page(topic):\n",
    "    \n",
    "    base_url = 'https://github.com/VincentBanuelos/nba_salary_predictor'\n",
    "    \n",
    "    response = get(base_url + topic)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    project_title = soup.find('a', name='user-content-top')\n",
    "    \n",
    "    summaries = soup.find('div', itemprop='articleBody')\n",
    "    \n",
    "    summary_list = []\n",
    "    \n",
    "    for i in range(len(titles)):\n",
    "        \n",
    "        temp_dict = {}\n",
    "        \n",
    "        temp_dict['project_title'] = project_title.text\n",
    "        \n",
    "        temp_dict['content'] = summaries[i].text\n",
    "        \n",
    "        temp_dict['category'] = topic\n",
    "        \n",
    "        summary_list.append(temp_dict)\n",
    "        \n",
    "    return summary_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://github.com/VincentBanuelos/nba_salary_predictor'\n",
    "\n",
    "response = get(base_url)\n",
    "    \n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "readmebox = soup.find('div', id='readme').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPredicting NBA Salaries using Regression\\nProject Description/Goals:\\nInitial Questions:\\nPlanning:\\nData Dictionary\\nReproduction Requirements:\\nPipeline Conclusions and Takeaways:\\nWrangling Takeaways\\nExploration Summary\\nModeling takeaways\\nConclusion, and Next Steps:\\n\\n\\n\\n\\n\\nREADME.md\\n\\n\\n\\n\\nPredicting NBA Salaries using Regression\\n\\nby: Vincent Banuelos\\n\\n[Project Description/Goals]\\n[Initial Questions]\\n[Planning]\\n[Data Dictionary]\\n[Reproduction Requirements]\\n[Pipeline Takeaways]\\n[Conclusion]\\n\\nProject Description/Goals:\\n\\n\\nUsing both basic stats and advanced stats can I predict an NBA player's salary as well as what which stat(s) are the biggest driver's of an NBA players Salary.\\n\\n\\nThis project runs through the entire Data Science Pipeline using regression models to attmept to predict NBA Players' salaries.\\n\\n\\n[Back to top]\\nInitial Questions:\\n\\nDoes the county a property is located in affect it's log error?\\nDoes the tax variables of a house affect the logerror?\\nDoes the ratio of home sqft to lot sqft affect logerror?\\nDoes the year a house was built affect logerror?\\n\\n[Back to top]\\nPlanning:\\n\\nCreate README.md with data dictionary, project goals, and come up with initial hypotheses.\\nAcquire data from the Basketball Reference website, turn into a CSV and create a function to automate this process.\\nClean and prepare data for the first iteration through the pipeline, MVP preparation. Create a function to automate the process.\\nStore the acquisition and preparation functions in a wrangle.py module function, and prepare data in Final Report Notebook by importing and using the function.\\nClearly define at least two hypotheses, set an alpha, run the statistical tests needed, reject or fail to reject the Null Hypothesis, and document findings and takeaways.\\nEstablish a baseline accuracy and document well.\\nTrain at least 3 different regression models.\\nEvaluate models on train and validate datasets.\\nChoose the model that performs the best and evaluate that single model on the test dataset.\\nDocument conclusions, takeaways, and next steps in the Final Report Notebook.\\n\\n[Back to top]\\nData Dictionary\\n\\n\\n\\nTarget Attribute\\nDefinition\\nData Type\\n\\n\\n\\n\\nsalary\\nThe NBA Player's salary for the 2017-2018 season\\nfloat\\n\\n\\n\\n\\n\\n\\n\\nFeature\\nDefinition\\nData Type\\n\\n\\n\\n\\nage\\nPlayer's Age on Feb 1 of the season\\nfloat\\n\\n\\ngp\\nGames Played in 2017-2018 season\\nfloat\\n\\n\\ngs\\nGames Started in 2017-2018 season\\nfloat\\n\\n\\nmp\\nMinutes Played in 2017-2018 season\\nfloat\\n\\n\\nfg\\nNumber of Field Goals made in 2017-2018 season\\nfloat\\n\\n\\nfga\\nNumber of Field Goals attempted in 2017-2018 season\\nfloat\\n\\n\\n2p\\nNumber of 2-pointers made in 2017-2018 season\\nfloat\\n\\n\\n2pa\\nNumber of 2-pointers attempted in 2017-2018 season\\nfloat\\n\\n\\n3p\\nNumber of 3-pointers made in 2017-2018 season\\nfloat\\n\\n\\n3pa\\nNumber of 3-pointers attempted in 2017-2018 season\\nfloat\\n\\n\\nft\\nNumber of Freethrows made in 2017-2018 season\\nfloat\\n\\n\\nfta\\nNumber of Freethrows attempted in 2017-2018 season\\nfloat\\n\\n\\norb\\nOffensive rebounds per game\\nfloat\\n\\n\\ndrb\\nDefensive rebounds per game\\nfloat\\n\\n\\ntrb\\nTotal rebounds per game\\nfloat\\n\\n\\nast\\nAsists per game\\nfloat\\n\\n\\nstl\\nSteals per game\\nfloat\\n\\n\\nblk\\nBlocks per game\\nfloat\\n\\n\\ntov\\nTurnovers per game\\nfloat\\n\\n\\npf\\nPersonal Fouls per game\\nfloat\\n\\n\\nppg\\nPoints per game\\nfloat\\n\\n\\nfg_pct\\nField Goal Percentage\\nfloat\\n\\n\\n2p_pct\\n2 Point Field Goal Percentage\\nfloat\\n\\n\\n3p_pct\\n3 Point Field Goal Percentage\\nfloat\\n\\n\\nft_pct\\nFreethrow Percentage\\nfloat\\n\\n\\nts_pct\\nTrue Shooting Percentage, True shooting percentage is a measure of shooting efficiency that takes into account field goals, 3-point field goals, and free throws.\\nfloat\\n\\n\\nefg_pct\\nEffective Field Goal Percentage, This statistic adjusts for the fact that a 3-point field goal is worth one more point than a 2-point field goal\\nfloat\\n\\n\\npos\\nPlayer's position\\nobject\\n\\n\\nws\\nWin Shares; an estimate of the number of wins contributed by a player.\\nfloat\\n\\n\\nortg\\nOffensive Rating for players it is points produced per 100 posessions\\nfloat\\n\\n\\ndrtg\\nDefensive Rating for players it is points allowed per 100 posessions.\\nfloat\\n\\n\\nows\\nOffensive Win Shares\\nfloat\\n\\n\\ndws\\nDefensive Win Shares\\nfloat\\n\\n\\nbpm\\nBox Plus/Minus a box score estimate of the points per 100 possessions that a player contributed above a league-average player, translated to an average team.\\nfloat\\n\\n\\nobpm\\nOffensive Box Plus/Minus\\nfloat\\n\\n\\ndbpm\\nOffensive Box Plus/Minus\\nfloat\\n\\n\\nvorp\\nValue Over Replacement Player; a box score estimate of the points per 100 TEAM possessions that a player contributed above a replacement-level (-2.0) player, translated to an average team and prorated to an 82-game season.\\nfloat\\n\\n\\nper\\nPlayer Efficiency Rating; PER sums up all a player's positive accomplishments, subtracts the negative accomplishments, and returns a per-minute rating of a player's performance.\\nfloat\\n\\n\\norb_pct\\nAn estimate of the percentage of available offensive rebounds a player grabbed while they were on the floor\\nfloat\\n\\n\\ndrb_pct\\nAn estimate of the percentage of available defensive rebounds a player grabbed while they were on the floor\\nfloat\\n\\n\\ntrb_pct\\nAn estimate of the percentage of available rebounds a player grabbed while they were on the floor\\nfloat\\n\\n\\nast_pct\\nAssist percentage is an estimate of the percentage of teammate field goals a player assisted while he was on the floor.\\nfloat\\n\\n\\nstl_pct\\nSteal Percentage is an estimate of the percentage of opponent possessions that end with a steal by the player while he was on the floor.\\nfloat\\n\\n\\nblk_pct\\nBlock percentage is an estimate of the percentage of opponent two-point field goal attempts blocked by the player while he was on the floor.\\nfloat\\n\\n\\ntov_pct\\nTurnover percentage is an estimate of turnovers per 100 plays.\\nfloat\\n\\n\\nusg_pct\\nUsage percentage is an estimate of the percentage of team plays used by a player while he was on the floor.\\nfloat\\n\\n\\nabove_avg_scorer\\n1 if a player is above the league average in scoring. 0 if not above average.\\nfloat\\n\\n\\nabove_avg_3ball\\n1 if a player is above the league average in 3pt %. 0 if not above average.\\nfloat\\n\\n\\nabove_avg_ft\\n1 if a player is above the league average in Freethrow %. 0 if not above average.\\nfloat\\n\\n\\nabove_avg_usg_pct\\n1 if a player is above the league average in Usage %. 0 if not above average.\\nfloat\\n\\n\\nC\\n1 if a player is a Center. 0 if not.\\nfloat\\n\\n\\nF\\n1 if a player is a Forward. 0 if not.\\nfloat\\n\\n\\nG\\n1 if a player is a Guard. 0 if not.\\nfloat\\n\\n\\n\\n\\nReproduction Requirements:\\nYou will need your own env.py file with database credentials then follow the steps below:\\n\\nDownload the csv files, allplayers_wrangle.py, model.py, explore.py, and final_report.ipynb files\\nRun the final_report.ipynb notebook\\n\\n[Back to top]\\nPipeline Conclusions and Takeaways:\\nWrangling Takeaways\\n\\nUsing data from basketball reference we pulled in players' statistical and salary data from the 2017-2018 NBA Season.\\nFollowing the Data Acquisition the following preparation work was done to the acquired data:\\n\\nRemoved any players who did have complete statistical lines or salaries from the dataset.\\nCreated features that compare and find players who are above average in a few commonly used stats.\\nFollowing data prepartion, we were left with a dataframe consisting of 415 observations and 55 statistical columns.\\nSplit data into 3 datasets, train, validate and test\\n\\n\\n\\nExploration Summary\\n\\n\\nPlayers with an above avg TS% and PPG DO earn a significantly different salary than then the rest of the league.\\n\\n\\nPlayers with an above avg USG % DO earn a significantly different salary than then the rest of the league.\\n\\n\\nPlayers with an above avg VORP DO earn a significantly different salary than then the rest of the league.\\n\\n\\nWe saw that a position that a player pays does have a impact on their salary.\\n\\n\\nModeling takeaways\\n\\n\\nThe Tweedie Regressor model did the most consistent out of all the models tested and did not overfit.\\n\\n\\nWith the model for all three data sets roughly falling 4-5 million dollars off of a players actual salary.\\n\\n\\n[Back to top]\\nConclusion, and Next Steps:\\n\\n\\nAlthough this number seems far off without context it must be remembered that there are factors at play that simply cannot be measured by statistics alone.\\n\\n\\nFactors such as player popularity in the team's fanbase, a player's potential to improve, the market value of a player with similar skillsets, whether a player is willing to take a pay cut for a better chance at winning, personal achievements by a player such as MVP, ALL-NBA, ALL-DEFENSE, All-Star selections, whether a player has been injured, etc.\\n\\n\\nFor the next steps after this project I would like to incorporate a player's personal achievements into consideration and see how much those achievements affect players' salaries.\\n\\n\\nAs well as pulling in shot chart data that shows a player's shot distribution, seeing as how certain players may perform best in certain roles, such as a spot-up shooter, pick and roll maestro or rollman, etc.\\n\\n\\nWith the NBA being an everchanging league, certain players will fill roles that are seen as better contributors to winning, and thus salaries will continue to fluctuate for players outside or the elite of the elite players.\\n\\n\\nIn conclusion, I believe my models provided a good starting point for giving a player an initial offer based solely on statistics alone.\\n\\n\\n[Back to top]\\n\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readmebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://github.com/VincentBanuelos/nba_salary_predictor'\n",
    "\n",
    "response = get(base_url)\n",
    "    \n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "gl = soup.find_all('span', class_='color-fg-default text-bold mr-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"color-fg-default text-bold mr-1\">Jupyter Notebook</span>,\n",
       " <span class=\"color-fg-default text-bold mr-1\">Python</span>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "gl = soup.find_all('span', class_='color-fg-default text-bold mr-1')\n",
    "for i in gl:\n",
    "    lst.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jupyter Notebook', 'Python']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://github.com/dragonflydb/dragonfly'\n",
    "\n",
    "response = get(base_url)\n",
    "    \n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "gl = soup.find_all('span', class_='color-fg-default text-bold mr-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"color-fg-default text-bold mr-1\">C++</span>,\n",
       " <span class=\"color-fg-default text-bold mr-1\">C</span>,\n",
       " <span class=\"color-fg-default text-bold mr-1\">CMake</span>,\n",
       " <span class=\"color-fg-default text-bold mr-1\">Python</span>,\n",
       " <span class=\"color-fg-default text-bold mr-1\">Shell</span>,\n",
       " <span class=\"color-fg-default text-bold mr-1\">Smarty</span>,\n",
       " <span class=\"color-fg-default text-bold mr-1\">Dockerfile</span>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "gl = soup.find_all('span', class_='color-fg-default text-bold mr-1')\n",
    "for i in gl:\n",
    "    lst.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C++', 'C', 'CMake', 'Python', 'Shell', 'Smarty', 'Dockerfile']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://github.com/VincentBanuelos/nba_salary_predictor','https://github.com/dragonflydb/dragonfly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to collect the information and cache it as a json file\n",
    "def get_readme(article_list):\n",
    "    file = 'readme_s.json'\n",
    "    \n",
    "    if os.path.exists(file):\n",
    "        \n",
    "        with open(file) as f:\n",
    "        \n",
    "            return json.load(f)\n",
    "        \n",
    "    github_info = []\n",
    "\n",
    "    for article in article_list:\n",
    "            \n",
    "        github_dict = {}\n",
    "        \n",
    "        response = get(article)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        github_dict['readme_txt'] = soup.find('div', id='readme').text\n",
    "        \n",
    "        list = []\n",
    "        lang = soup.find_all('span', class_='color-fg-default text-bold mr-1')\n",
    "        for i in lang:\n",
    "            list.append(i.text)\n",
    "\n",
    "        if list[0] == 'Jupyter Notebook':\n",
    "            github_dict['languages'] = list[1]\n",
    "        \n",
    "        else:\n",
    "            github_dict['languages'] = list[0]\n",
    "        \n",
    "        github_info.append(github_dict)\n",
    "\n",
    "        with open(file, 'w') as f:\n",
    "        \n",
    "            json.dump(github_info, f)\n",
    "        \n",
    "    return github_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'readme_txt': \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPredicting NBA Salaries using Regression\\nProject Description/Goals:\\nInitial Questions:\\nPlanning:\\nData Dictionary\\nReproduction Requirements:\\nPipeline Conclusions and Takeaways:\\nWrangling Takeaways\\nExploration Summary\\nModeling takeaways\\nConclusion, and Next Steps:\\n\\n\\n\\n\\n\\nREADME.md\\n\\n\\n\\n\\nPredicting NBA Salaries using Regression\\n\\nby: Vincent Banuelos\\n\\n[Project Description/Goals]\\n[Initial Questions]\\n[Planning]\\n[Data Dictionary]\\n[Reproduction Requirements]\\n[Pipeline Takeaways]\\n[Conclusion]\\n\\nProject Description/Goals:\\n\\n\\nUsing both basic stats and advanced stats can I predict an NBA player's salary as well as what which stat(s) are the biggest driver's of an NBA players Salary.\\n\\n\\nThis project runs through the entire Data Science Pipeline using regression models to attmept to predict NBA Players' salaries.\\n\\n\\n[Back to top]\\nInitial Questions:\\n\\nDoes the county a property is located in affect it's log error?\\nDoes the tax variables of a house affect the logerror?\\nDoes the ratio of home sqft to lot sqft affect logerror?\\nDoes the year a house was built affect logerror?\\n\\n[Back to top]\\nPlanning:\\n\\nCreate README.md with data dictionary, project goals, and come up with initial hypotheses.\\nAcquire data from the Basketball Reference website, turn into a CSV and create a function to automate this process.\\nClean and prepare data for the first iteration through the pipeline, MVP preparation. Create a function to automate the process.\\nStore the acquisition and preparation functions in a wrangle.py module function, and prepare data in Final Report Notebook by importing and using the function.\\nClearly define at least two hypotheses, set an alpha, run the statistical tests needed, reject or fail to reject the Null Hypothesis, and document findings and takeaways.\\nEstablish a baseline accuracy and document well.\\nTrain at least 3 different regression models.\\nEvaluate models on train and validate datasets.\\nChoose the model that performs the best and evaluate that single model on the test dataset.\\nDocument conclusions, takeaways, and next steps in the Final Report Notebook.\\n\\n[Back to top]\\nData Dictionary\\n\\n\\n\\nTarget Attribute\\nDefinition\\nData Type\\n\\n\\n\\n\\nsalary\\nThe NBA Player's salary for the 2017-2018 season\\nfloat\\n\\n\\n\\n\\n\\n\\n\\nFeature\\nDefinition\\nData Type\\n\\n\\n\\n\\nage\\nPlayer's Age on Feb 1 of the season\\nfloat\\n\\n\\ngp\\nGames Played in 2017-2018 season\\nfloat\\n\\n\\ngs\\nGames Started in 2017-2018 season\\nfloat\\n\\n\\nmp\\nMinutes Played in 2017-2018 season\\nfloat\\n\\n\\nfg\\nNumber of Field Goals made in 2017-2018 season\\nfloat\\n\\n\\nfga\\nNumber of Field Goals attempted in 2017-2018 season\\nfloat\\n\\n\\n2p\\nNumber of 2-pointers made in 2017-2018 season\\nfloat\\n\\n\\n2pa\\nNumber of 2-pointers attempted in 2017-2018 season\\nfloat\\n\\n\\n3p\\nNumber of 3-pointers made in 2017-2018 season\\nfloat\\n\\n\\n3pa\\nNumber of 3-pointers attempted in 2017-2018 season\\nfloat\\n\\n\\nft\\nNumber of Freethrows made in 2017-2018 season\\nfloat\\n\\n\\nfta\\nNumber of Freethrows attempted in 2017-2018 season\\nfloat\\n\\n\\norb\\nOffensive rebounds per game\\nfloat\\n\\n\\ndrb\\nDefensive rebounds per game\\nfloat\\n\\n\\ntrb\\nTotal rebounds per game\\nfloat\\n\\n\\nast\\nAsists per game\\nfloat\\n\\n\\nstl\\nSteals per game\\nfloat\\n\\n\\nblk\\nBlocks per game\\nfloat\\n\\n\\ntov\\nTurnovers per game\\nfloat\\n\\n\\npf\\nPersonal Fouls per game\\nfloat\\n\\n\\nppg\\nPoints per game\\nfloat\\n\\n\\nfg_pct\\nField Goal Percentage\\nfloat\\n\\n\\n2p_pct\\n2 Point Field Goal Percentage\\nfloat\\n\\n\\n3p_pct\\n3 Point Field Goal Percentage\\nfloat\\n\\n\\nft_pct\\nFreethrow Percentage\\nfloat\\n\\n\\nts_pct\\nTrue Shooting Percentage, True shooting percentage is a measure of shooting efficiency that takes into account field goals, 3-point field goals, and free throws.\\nfloat\\n\\n\\nefg_pct\\nEffective Field Goal Percentage, This statistic adjusts for the fact that a 3-point field goal is worth one more point than a 2-point field goal\\nfloat\\n\\n\\npos\\nPlayer's position\\nobject\\n\\n\\nws\\nWin Shares; an estimate of the number of wins contributed by a player.\\nfloat\\n\\n\\nortg\\nOffensive Rating for players it is points produced per 100 posessions\\nfloat\\n\\n\\ndrtg\\nDefensive Rating for players it is points allowed per 100 posessions.\\nfloat\\n\\n\\nows\\nOffensive Win Shares\\nfloat\\n\\n\\ndws\\nDefensive Win Shares\\nfloat\\n\\n\\nbpm\\nBox Plus/Minus a box score estimate of the points per 100 possessions that a player contributed above a league-average player, translated to an average team.\\nfloat\\n\\n\\nobpm\\nOffensive Box Plus/Minus\\nfloat\\n\\n\\ndbpm\\nOffensive Box Plus/Minus\\nfloat\\n\\n\\nvorp\\nValue Over Replacement Player; a box score estimate of the points per 100 TEAM possessions that a player contributed above a replacement-level (-2.0) player, translated to an average team and prorated to an 82-game season.\\nfloat\\n\\n\\nper\\nPlayer Efficiency Rating; PER sums up all a player's positive accomplishments, subtracts the negative accomplishments, and returns a per-minute rating of a player's performance.\\nfloat\\n\\n\\norb_pct\\nAn estimate of the percentage of available offensive rebounds a player grabbed while they were on the floor\\nfloat\\n\\n\\ndrb_pct\\nAn estimate of the percentage of available defensive rebounds a player grabbed while they were on the floor\\nfloat\\n\\n\\ntrb_pct\\nAn estimate of the percentage of available rebounds a player grabbed while they were on the floor\\nfloat\\n\\n\\nast_pct\\nAssist percentage is an estimate of the percentage of teammate field goals a player assisted while he was on the floor.\\nfloat\\n\\n\\nstl_pct\\nSteal Percentage is an estimate of the percentage of opponent possessions that end with a steal by the player while he was on the floor.\\nfloat\\n\\n\\nblk_pct\\nBlock percentage is an estimate of the percentage of opponent two-point field goal attempts blocked by the player while he was on the floor.\\nfloat\\n\\n\\ntov_pct\\nTurnover percentage is an estimate of turnovers per 100 plays.\\nfloat\\n\\n\\nusg_pct\\nUsage percentage is an estimate of the percentage of team plays used by a player while he was on the floor.\\nfloat\\n\\n\\nabove_avg_scorer\\n1 if a player is above the league average in scoring. 0 if not above average.\\nfloat\\n\\n\\nabove_avg_3ball\\n1 if a player is above the league average in 3pt %. 0 if not above average.\\nfloat\\n\\n\\nabove_avg_ft\\n1 if a player is above the league average in Freethrow %. 0 if not above average.\\nfloat\\n\\n\\nabove_avg_usg_pct\\n1 if a player is above the league average in Usage %. 0 if not above average.\\nfloat\\n\\n\\nC\\n1 if a player is a Center. 0 if not.\\nfloat\\n\\n\\nF\\n1 if a player is a Forward. 0 if not.\\nfloat\\n\\n\\nG\\n1 if a player is a Guard. 0 if not.\\nfloat\\n\\n\\n\\n\\nReproduction Requirements:\\nYou will need your own env.py file with database credentials then follow the steps below:\\n\\nDownload the csv files, allplayers_wrangle.py, model.py, explore.py, and final_report.ipynb files\\nRun the final_report.ipynb notebook\\n\\n[Back to top]\\nPipeline Conclusions and Takeaways:\\nWrangling Takeaways\\n\\nUsing data from basketball reference we pulled in players' statistical and salary data from the 2017-2018 NBA Season.\\nFollowing the Data Acquisition the following preparation work was done to the acquired data:\\n\\nRemoved any players who did have complete statistical lines or salaries from the dataset.\\nCreated features that compare and find players who are above average in a few commonly used stats.\\nFollowing data prepartion, we were left with a dataframe consisting of 415 observations and 55 statistical columns.\\nSplit data into 3 datasets, train, validate and test\\n\\n\\n\\nExploration Summary\\n\\n\\nPlayers with an above avg TS% and PPG DO earn a significantly different salary than then the rest of the league.\\n\\n\\nPlayers with an above avg USG % DO earn a significantly different salary than then the rest of the league.\\n\\n\\nPlayers with an above avg VORP DO earn a significantly different salary than then the rest of the league.\\n\\n\\nWe saw that a position that a player pays does have a impact on their salary.\\n\\n\\nModeling takeaways\\n\\n\\nThe Tweedie Regressor model did the most consistent out of all the models tested and did not overfit.\\n\\n\\nWith the model for all three data sets roughly falling 4-5 million dollars off of a players actual salary.\\n\\n\\n[Back to top]\\nConclusion, and Next Steps:\\n\\n\\nAlthough this number seems far off without context it must be remembered that there are factors at play that simply cannot be measured by statistics alone.\\n\\n\\nFactors such as player popularity in the team's fanbase, a player's potential to improve, the market value of a player with similar skillsets, whether a player is willing to take a pay cut for a better chance at winning, personal achievements by a player such as MVP, ALL-NBA, ALL-DEFENSE, All-Star selections, whether a player has been injured, etc.\\n\\n\\nFor the next steps after this project I would like to incorporate a player's personal achievements into consideration and see how much those achievements affect players' salaries.\\n\\n\\nAs well as pulling in shot chart data that shows a player's shot distribution, seeing as how certain players may perform best in certain roles, such as a spot-up shooter, pick and roll maestro or rollman, etc.\\n\\n\\nWith the NBA being an everchanging league, certain players will fill roles that are seen as better contributors to winning, and thus salaries will continue to fluctuate for players outside or the elite of the elite players.\\n\\n\\nIn conclusion, I believe my models provided a good starting point for giving a player an initial offer based solely on statistics alone.\\n\\n\\n[Back to top]\\n\\n\\n\",\n",
       "  'languages': 'Python'},\n",
       " {'readme_txt': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProbably, the fastest in-memory store in the universe!\\nBenchmarks\\nMemcached / Dragonfly\\nSET benchmark\\nGET benchmark\\nMemory efficiency\\nRunning the server\\nWith docker:\\nReleases\\nBuilding from source\\nConfiguration\\nRoadmap and status\\nMilestone - H/A\\nMilestone - \"Maturity\"\\nNext milestones will be determined along the way.\\nDesign decisions\\nNovel cache design\\nExpiration deadlines with relative accuracy\\nNative Http console and Prometheus compatible metrics\\nBackground\\n\\n\\n\\n\\n\\nREADME.md\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nQuick Start | Discord Chat | GitHub Discussions | GitHub Issues | Contributing\\nProbably, the fastest in-memory store in the universe!\\nDragonfly is a modern in-memory datastore, fully compatible with Redis and Memcached APIs. Dragonfly implements novel algorithms and data structures on top of a multi-threaded, shared-nothing architecture. As a result, Dragonfly reaches x25 performance\\ncompared to Redis and supports millions of QPS on a single instance.\\nDragonfly\\'s core properties make it a cost-effective, high-performing, and easy-to-use Redis replacement.\\nBenchmarks\\n\\nDragonfly is crossing 3.8M QPS on c6gn.16xlarge reaching x25 increase in throughput compared to Redis.\\n99th latency percentile of Dragonfly at its peak throughput:\\n\\n\\n\\nop\\nr6g\\nc6gn\\nc7g\\n\\n\\n\\n\\nset\\n0.8ms\\n1ms\\n1ms\\n\\n\\nget\\n0.9ms\\n0.9ms\\n0.8ms\\n\\n\\nsetex\\n0.9ms\\n1.1ms\\n1.3ms\\n\\n\\n\\nAll benchmarks were performed using memtier_benchmark  (see below) with number of threads tuned per server type and the instance type. memtier was running on a separate c6gn.16xlarge machine. For setex benchmark we used expiry-range of 500, so it would survive the end of the test.\\n  memtier_benchmark --ratio ... -t <threads> -c 30 -n 200000 --distinct-client-seed -d 256 \\\\\\n     --expiry-range=...\\nWhen running in pipeline mode --pipeline=30, Dragonfly reaches 10M qps for SET and 15M qps for GET operations.\\nMemcached / Dragonfly\\nWe compared memcached with Dragonfly on c6gn.16xlarge instance on AWS.\\nAs you can see below Dragonfly dominates memcached for both write and read workloads\\nin terms of throughput with a comparable latency. For write workloads, Dragonfly has also better latency, due to contention on the write path in memcached.\\nSET benchmark\\n\\n\\n\\nServer\\nQPS(thousands qps)\\nlatency 99%\\n99.9%\\n\\n\\n\\n\\nDragonfly\\nüü© 3844\\nüü© 0.9ms\\nüü© 2.4ms\\n\\n\\nMemcached\\n806\\n1.6ms\\n3.2ms\\n\\n\\n\\nGET benchmark\\n\\n\\n\\nServer\\nQPS(thousands qps)\\nlatency 99%\\n99.9%\\n\\n\\n\\n\\nDragonfly\\nüü© 3717\\n1ms\\n2.4ms\\n\\n\\nMemcached\\n2100\\nüü© 0.34ms\\nüü© 0.6ms\\n\\n\\n\\nMemcached exhibited lower latency for the read benchmark, but also lower throughput.\\nMemory efficiency\\nIn the following test, we filled Dragonfly and Redis with ~5GB of data\\nusing debug populate 5000000 key 1024 command. Then we started sending the update traffic with memtier and kicked off the snapshotting with the\\n\"bgsave\" command. The following figure demonstrates clearly how both servers behave in terms of memory efficiency.\\n\\nDragonfly was 30% more memory efficient than Redis at the idle state.\\nIt also did not show any visible memory increase during the snapshot phase.\\nMeanwhile, Redis reached almost x3 memory increase at peak compared to Dragonfly.\\nDragonfly also finished the snapshot much faster, just a few seconds after it started.\\nFor more info about memory efficiency in Dragonfly see dashtable doc\\nRunning the server\\nDragonfly runs on linux. We advice running it on linux version 5.11 or later\\nbut you can also run Dragonfly on older kernels as well.\\nWith docker:\\ndocker run --network=host --ulimit memlock=-1 docker.dragonflydb.io/dragonflydb/dragonfly\\n\\nredis-cli PING  # redis-cli can be installed with \"apt install -y redis-tools\"\\nYou need --ulimit memlock=-1 because some Linux distros configure the default memlock limit for containers as 64m and Dragonfly requires more.\\nReleases\\nWe maintain binary releases for x86 and arm64 architectures. You will need to install libunwind8 lib to run the binaries.\\nBuilding from source\\nYou need to install dependencies in order to build on Ubuntu 20.04 or later:\\ngit clone --recursive https://github.com/dragonflydb/dragonfly && cd dragonfly\\n\\n# to install dependencies\\nsudo apt install ninja-build libunwind-dev libboost-fiber-dev libssl-dev \\\\\\n     autoconf-archive libtool cmake g++\\n\\n# Configure the build\\n./helio/blaze.sh -release\\n\\n# Build\\ncd build-opt && ninja dragonfly\\n\\n# Run\\n./dragonfly --alsologtostderr\\n\\nConfiguration\\nDragonfly supports common redis arguments where applicable.\\nFor example, you can run: dragonfly --requirepass=foo --bind localhost.\\nDragonfly currently supports the following Redis-specific arguments:\\n\\nport redis connection port, default: 6379\\nbind localhost to only allow locahost connections, Public IP ADDRESS , to allow connections to that ip address (aka from outside too)\\nrequirepass password for AUTH authentication, default: \"\"\\nmaxmemory Limit on maximum-memory (in bytes) that is used by the database.0 - means the program will automatically determine its maximum memory usage. default: 0\\ndir - by default, dragonfly docker uses /data folder for snapshotting. the CLI uses: \"\"\\nYou can use -v docker option to map it to your host folder.\\ndbfilename the filename to save/load the DB. default: \"dump\";\\n\\nIn addition, it has Dragonfly specific arguments options:\\n\\nmemcache_port  - to enable memcached compatible API on this port. Disabled by default.\\nkeys_output_limit - maximum number of returned keys in keys command. Default is 8192.\\nkeys is a dangerous command. We truncate its result to avoid blowup in memory when fetching too many keys.\\ndbnum - maximum number of supported databases for select.\\ncache_mode - see Cache section below.\\nhz - key expiry evaluation frequency. Default is 1000. Lower frequency uses less cpu when\\nidle at the expense of precision in key eviction.\\nsave_schedule - glob spec for the UTC time to save a snapshot which matches HH:MM (24h time). default: \"\"\\nkeys_output_limit -  Maximum number of keys output by keys command. default: 8192\\n\\nfor more options like logs management or tls support, run dragonfly --help.\\nRoadmap and status\\nCurrently Dragonfly supports ~130 Redis commands and all memcache commands besides cas.\\nWe are almost on par with Redis 2.8 API. Our first milestone will be to stabilize basic\\nfunctionality and reach API parity with Redis 2.8 and Memcached APIs.\\nIf you see that a command you need, is not implemented yet, please open an issue.\\nThe next milestone will be implementing H/A with redis -> dragonfly and\\ndragonfly<->dragonfly replication.\\nFor dragonfly-native replication, we are planning to design a distributed log format that will\\nsupport order of magnitude higher speeds when replicating.\\nAfter replication and failover feature we will continue with other Redis commands from\\nAPIs 3,4 and 5.\\nPlease see API readiness doc for the current status of Dragonfly.\\nMilestone - H/A\\nImplement leader/follower replication (PSYNC/REPLICAOF/...).\\nMilestone - \"Maturity\"\\nAPIs 3,4,5 without cluster support, without modules and without memory introspection commands. Also\\nwithout geo commands and without support for keyspace notifications, without streams.\\nProbably design config support. Overall - few dozens commands...\\nProbably implement cluster-API decorators to allow cluster-configured clients to connect to a\\nsingle instance.\\nNext milestones will be determined along the way.\\nDesign decisions\\nNovel cache design\\nDragonfly has a single unified adaptive caching algorithm that is very simple and memory efficient.\\nYou can enable caching mode by passing --cache_mode=true flag. Once this mode\\nis on, Dragonfly will evict items least likely to be stumbled upon in the future but only when\\nit is near maxmemory limit.\\nExpiration deadlines with relative accuracy\\nExpiration ranges are limited to ~4 years. Moreover, expiration deadlines\\nwith millisecond precision (PEXPIRE/PSETEX etc) will be rounded to closest second\\nfor deadlines greater than 134217727ms (approximately 37 hours).\\nSuch rounding has less than 0.001% error which I hope is acceptable for large ranges.\\nIf it breaks your use-cases - talk to me or open an issue and explain your case.\\nFor more detailed differences between this and Redis implementations see here.\\nNative Http console and Prometheus compatible metrics\\nBy default Dragonfly allows http access via its main TCP port (6379). That\\'s right, you\\ncan connect to Dragonfly via Redis protocol and via HTTP protocol - the server recognizes\\nthe protocol  automatically during the connection initiation. Go ahead and try it with your browser.\\nRight now it does not have much info but in the future we are planning to add there useful\\ndebugging and management info. If you go to :6379/metrics url you will see some prometheus\\ncompatible metrics.\\nThe Prometheus exported metrics are compatible with the Grafana dashboard see here.\\nImportant! Http console is meant to be accessed within a safe network.\\nIf you expose Dragonfly\\'s TCP port externally, it is advised to disable the console\\nwith --http_admin_console=false or --nohttp_admin_console.\\nBackground\\nDragonfly started as an experiment to see how an in-memory datastore could look like if it was designed in 2022. Based on  lessons learned from our experience as users of memory stores and as engineers who worked for cloud companies, we knew that we need to preserve two key properties for Dragonfly: a) to provide atomicity guarantees for all its operations, and b) to guarantee low, sub-millisecond latency over very high throughput.\\nOur first challenge was how to fully utilize CPU, memory, and i/o resources using servers that are available today in public clouds. To solve this, we used shared-nothing architecture, which allows us to partition the keyspace of the memory store between threads, so that each thread would manage its own slice of dictionary data. We call these slices - shards. The library that powers thread and I/O management for shared-nothing architecture is open-sourced here.\\nTo provide atomicity guarantees for multi-key operations, we used the advancements from recent academic research. We chose the paper \"VLL: a lock manager redesign for main memory database systems‚Äù to develop the transactional framework for Dragonfly. The choice of shared-nothing architecture and VLL allowed us to compose atomic multi-key operations without using mutexes or spinlocks. This was a major milestone for our PoC and its performance stood out from other commercial and open-source solutions.\\nOur second challenge was to engineer more efficient data structures for the new store. To achieve this goal, we based our core hashtable structure on paper \"Dash: Scalable Hashing on Persistent Memory\". The paper itself is centered around persistent memory domain and is not directly related to main-memory stores.\\nNevertheless, its very much applicable for our problem. It suggested a hashtable design that allowed us to maintain two special properties that are present in the Redis dictionary: a) its incremental hashing ability during datastore growth b) its ability to traverse the dictionary under changes using a stateless scan operation. Besides these 2 properties,\\nDash is much more efficient in CPU and memory. By leveraging Dash\\'s design, we were able to innovate further with the following features:\\n\\nEfficient record expiry for TTL records.\\nA novel cache eviction algorithm that achieves higher hit rates than other caching strategies like LRU and LFU with zero memory overhead.\\nA novel fork-less snapshotting algorithm.\\n\\nAfter we built the foundation for Dragonfly and we were happy with its performance,\\nwe went on to implement the Redis and Memcached functionality. By now, we have implemented ~130 Redis commands (equivalent to v2.8) and 13 Memcached commands.\\nAnd finally, \\nOur mission is to build a well-designed, ultra-fast, cost-efficient in-memory datastore for cloud workloads that takes advantage of the latest hardware advancements. We intend to address the pain points of current solutions while preserving their product APIs and propositions.\\n\\n\\n\\n',\n",
       "  'languages': 'C++'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_readme(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
